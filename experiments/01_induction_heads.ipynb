{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering Induction Heads in Small Language Models\n",
    "\n",
    "**Goal**: Replicate Anthropic's discovery of induction heads using our interpretability toolkit.\n",
    "\n",
    "**What are induction heads?**\n",
    "\n",
    "Induction heads are circuits in transformers that enable in-context learning. They implement a pattern-matching algorithm:\n",
    "\n",
    "```\n",
    "Given sequence: [A][B]...[A] → Predict [B]\n",
    "```\n",
    "\n",
    "When the model sees a repeated token [A], an induction head attends back to what came after the previous occurrence of [A], allowing it to predict [B].\n",
    "\n",
    "**References**:\n",
    "- [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) (Anthropic, 2022)\n",
    "- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) (Anthropic, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import nanoGPT\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "# Import our interpretability toolkit\n",
    "from interpretability import activation_patching, attention_analysis, logit_lens\n",
    "from interpretability.utils import ActivationCache, HookManager\n",
    "\n",
    "# Notebook settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load or Train a Model\n",
    "\n",
    "For this experiment, we'll use a small character-level GPT trained on Shakespeare.\n",
    "This model is small enough to analyze thoroughly but large enough to exhibit interesting behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Load a pretrained checkpoint\n",
    "checkpoint_path = '../trained_models/shakespeare_char_model.pt'\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Create model from checkpoint\n",
    "    model_args = checkpoint['model_args']\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded: {model.get_num_params()/1e6:.2f}M parameters\")\n",
    "    \n",
    "else:\n",
    "    print(\"No checkpoint found. You'll need to train a model first.\")\n",
    "    print(\"Run: python train.py config/train_shakespeare_char.py --max_iters=5000\")\n",
    "    print(\"Then copy the checkpoint to trained_models/shakespeare_char_model.pt\")\n",
    "    \n",
    "    # For demonstration, create a small model (won't have learned induction yet)\n",
    "    print(\"\\nCreating untrained model for demonstration...\")\n",
    "    config = GPTConfig(\n",
    "        block_size=256,\n",
    "        vocab_size=65,  # Shakespeare character vocab\n",
    "        n_layer=6,\n",
    "        n_head=6,\n",
    "        n_embd=384,\n",
    "        dropout=0.0,\n",
    "        bias=False\n",
    "    )\n",
    "    model = GPT(config)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Note: Untrained model won't show induction head behavior!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Test Sequences for Induction\n",
    "\n",
    "We'll create sequences with repeated patterns to test for induction behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple induction test sequences\n",
    "# Pattern: [A][B][C][A][?] -> Should predict [B]\n",
    "\n",
    "def create_induction_sequence(vocab_size=65, seq_len=20, repeat_at=10):\n",
    "    \"\"\"\n",
    "    Create a sequence with a repeated pattern for testing induction.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        seq_len: Total sequence length\n",
    "        repeat_at: Where to start the repeat pattern\n",
    "    \n",
    "    Returns:\n",
    "        sequence: Tensor of token IDs\n",
    "        repeat_token: The token that gets repeated\n",
    "        target_token: The token that should be predicted after repeat\n",
    "    \"\"\"\n",
    "    # Create random initial sequence\n",
    "    sequence = torch.randint(0, vocab_size, (seq_len,))\n",
    "    \n",
    "    # Pick a position before repeat_at to copy\n",
    "    copy_from = repeat_at // 2\n",
    "    \n",
    "    # Copy pattern: sequence[copy_from] appears at repeat_at\n",
    "    # We want to test if model predicts sequence[copy_from+1]\n",
    "    sequence[repeat_at] = sequence[copy_from]\n",
    "    \n",
    "    repeat_token = sequence[copy_from].item()\n",
    "    target_token = sequence[copy_from + 1].item()\n",
    "    \n",
    "    return sequence.unsqueeze(0), repeat_token, target_token\n",
    "\n",
    "# Create test sequence\n",
    "test_seq, repeat_tok, target_tok = create_induction_sequence(\n",
    "    vocab_size=model.config.vocab_size,\n",
    "    seq_len=30,\n",
    "    repeat_at=20\n",
    ")\n",
    "\n",
    "print(f\"Test sequence: {test_seq.squeeze().tolist()}\")\n",
    "print(f\"Repeat token: {repeat_tok}\")\n",
    "print(f\"Target token (should be predicted): {target_tok}\")\n",
    "print(f\"\\nSequence visualization:\")\n",
    "print(f\"Position 10: {test_seq[0, 10].item()} (A)\")\n",
    "print(f\"Position 11: {test_seq[0, 11].item()} (B) <- This is what we want to predict\")\n",
    "print(f\"Position 20: {test_seq[0, 20].item()} (A again)\")\n",
    "print(f\"Position 21: ??? <- Model should predict (B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model's Induction Capability\n",
    "\n",
    "First, let's see if the model actually performs induction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "test_seq_device = test_seq.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, _ = model(test_seq_device)\n",
    "    \n",
    "# Get prediction at position after the repeat\n",
    "next_token_logits = logits[0, -1, :]  # Last position\n",
    "probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Get top 5 predictions\n",
    "top_probs, top_tokens = torch.topk(probs, 5)\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for i, (token, prob) in enumerate(zip(top_tokens, top_probs)):\n",
    "    marker = \" ← TARGET!\" if token.item() == target_tok else \"\"\n",
    "    print(f\"{i+1}. Token {token.item()}: {prob.item():.2%}{marker}\")\n",
    "\n",
    "# Check if target is in top 5\n",
    "if target_tok in top_tokens:\n",
    "    rank = (top_tokens == target_tok).nonzero(as_tuple=True)[0].item() + 1\n",
    "    prob = probs[target_tok].item()\n",
    "    print(f\"\\n✓ Model shows induction! Target token ranked #{rank} with {prob:.2%} probability\")\n",
    "else:\n",
    "    print(f\"\\n✗ Model did not predict target token in top 5\")\n",
    "    print(f\"  Target probability: {probs[target_tok].item():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find Induction Heads Using Activation Patching\n",
    "\n",
    "Now we'll use activation patching to systematically identify which layers and heads are responsible for induction behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean and corrupted inputs\n",
    "# Clean: normal induction sequence\n",
    "# Corrupted: sequence where repeat pattern is broken\n",
    "\n",
    "clean_seq, repeat_tok, target_tok = create_induction_sequence()\n",
    "corrupted_seq = clean_seq.clone()\n",
    "# Break the pattern by changing the repeated token\n",
    "corrupted_seq[0, 20] = (corrupted_seq[0, 20] + 5) % model.config.vocab_size\n",
    "\n",
    "clean_seq = clean_seq.to(device)\n",
    "corrupted_seq = corrupted_seq.to(device)\n",
    "\n",
    "print(\"Running activation patching scan...\")\n",
    "print(\"This identifies which layers are important for induction.\\n\")\n",
    "\n",
    "# Scan all transformer layers\n",
    "results = activation_patching.patch_layer_scan(\n",
    "    model,\n",
    "    clean_input=clean_seq,\n",
    "    corrupted_input=corrupted_seq,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Find most important layers\n",
    "important = activation_patching.find_important_components(\n",
    "    results,\n",
    "    threshold=0.3,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"\\nMost important layers for induction:\")\n",
    "for layer_name, effect in important:\n",
    "    print(f\"  {layer_name}: {effect:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Activation Patching Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot patching effects by layer\n",
    "layer_effects = [(int(name.split('.')[2]), result.effect) \n",
    "                 for name, result in results.items()]\n",
    "layer_effects.sort(key=lambda x: x[0])\n",
    "\n",
    "layers = [x[0] for x in layer_effects]\n",
    "effects = [x[1] for x in layer_effects]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(layers, effects, color='steelblue', alpha=0.7)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='High importance threshold')\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Patching Effect', fontsize=12)\n",
    "plt.title('Layer Importance for Induction (Activation Patching)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/induction_patching_effects.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to results/induction_patching_effects.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Attention Patterns\n",
    "\n",
    "Let's look at attention patterns to visually identify induction heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is a placeholder showing the intended analysis\n",
    "# The actual attention extraction needs to be implemented to work with nanoGPT's architecture\n",
    "\n",
    "print(\"Attention pattern analysis (to be implemented):\")\n",
    "print(\"- Extract attention weights from forward pass\")\n",
    "print(\"- Look for heads that attend to positions matching current token\")\n",
    "print(\"- Visualize attention patterns for suspected induction heads\")\n",
    "print(\"\\nThis requires modifying nanoGPT to expose attention weights,\")\n",
    "print(\"or using hooks to capture them during the forward pass.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logit Lens Analysis\n",
    "\n",
    "Use logit lens to see when the model \"decides\" on the induction prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running logit lens analysis...\\n\")\n",
    "\n",
    "# Analyze how prediction forms across layers\n",
    "lens_result = logit_lens.logit_lens(\n",
    "    model,\n",
    "    clean_seq,\n",
    "    target_position=-1,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Show prediction evolution\n",
    "print(\"Prediction evolution across layers:\")\n",
    "for layer_idx, predictions in enumerate(lens_result.predictions_by_layer):\n",
    "    top_pred, top_prob = predictions[0]\n",
    "    print(f\"Layer {layer_idx}: {top_pred} ({top_prob:.2%})\")\n",
    "\n",
    "# Measure convergence\n",
    "convergence = logit_lens.measure_convergence(lens_result, threshold=0.5)\n",
    "print(f\"\\nModel becomes confident at layer {convergence['convergence_layer']}\")\n",
    "print(f\"Prediction stability: {convergence['stability']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Prediction Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how predictions evolve\n",
    "fig = logit_lens.plot_prediction_evolution(\n",
    "    lens_result,\n",
    "    num_predictions=5,\n",
    "    save_path='../results/induction_logit_lens.png'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to results/induction_logit_lens.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Findings\n",
    "\n",
    "### Expected Results (on a trained model):\n",
    "\n",
    "1. **Induction Capability**: Model successfully predicts repeated patterns\n",
    "2. **Layer Localization**: Induction behavior emerges in middle-to-late layers (typically layers 3-5 in a 6-layer model)\n",
    "3. **Head Specialization**: Specific attention heads specialize in induction\n",
    "4. **Prediction Formation**: Logit lens shows gradual convergence to correct prediction\n",
    "\n",
    "### Comparison to Anthropic's Findings:\n",
    "\n",
    "Our results should align with:\n",
    "- Induction heads typically form after ~2000-5000 training iterations\n",
    "- Usually found in layers L/2 to 3L/4 (where L is total layers)\n",
    "- Responsible for significant portion of in-context learning capability\n",
    "- Can be identified by characteristic attention pattern\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Test on multiple random seeds to verify robustness\n",
    "2. Measure how induction capability develops during training\n",
    "3. Ablate suspected induction heads to measure their importance\n",
    "4. Compare to larger models to see how behavior scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "results_dict = {\n",
    "    'model_config': model.config.__dict__,\n",
    "    'induction_test': {\n",
    "        'repeat_token': repeat_tok,\n",
    "        'target_token': target_tok,\n",
    "        'predicted_correctly': target_tok in top_tokens,\n",
    "    },\n",
    "    'important_layers': important,\n",
    "    'convergence': convergence,\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "with open('../results/induction_heads_analysis.json', 'w') as f:\n",
    "    # Convert non-serializable types\n",
    "    results_serializable = {\n",
    "        k: v if not isinstance(v, (torch.Tensor, np.ndarray)) else str(v)\n",
    "        for k, v in results_dict.items()\n",
    "    }\n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "\n",
    "print(\"Results saved to results/induction_heads_analysis.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
